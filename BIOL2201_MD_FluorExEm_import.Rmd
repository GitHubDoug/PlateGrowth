---
title: "Import microbial plate growth data"
author: "Maximilian Berthold, Douglas A. Campbell"
date: "`r format(Sys.Date())`"
output:
  html_document:
    df_print: paged
    code_folding: hide
---

This .Rmd imports Molecular Device Fluorescence well plate files, generated using multiple excitation/emission wavelength pairs.
The size of the well plate does not matter; the import creates columns for RowColumn LetterNumber (A01 etc.) for the largest plate type in the list of files.
Import of smaller plate types results in NA in the non-existent RowColumn for that plate.

ToDo

Export simple Ex440nm Em680 nm trajectories for analyses in Excel
Assemble tidied multi-wavelength data for analyses in RStudio.cloud (Posit.cloud)


```{r load libraries, echo=FALSE, message = FALSE, warning = FALSE} 
# libraries; Note check actual dependencies
library(tidyverse) #core tidyverse packages
#library(minpack.lm) #curve fitting
#library(broom) #tidy model outputs
library(photobiology) #photobiology::w_length2rgb

```

Fix File Paths for Import once DataSets accumulate
```{r variable names for file import & processing MOLECULAR DEVICES}
Project <- "BIOL2201"
LabDay <- "TUES"

#File Ex Em nm settings lost from files during import b/c of Header structure

Em <- 680

Ex <- c(440, 520, 620) #Add colour coding vector
ExNM = (photobiology::w_length2rgb(Ex))
names(ExNM) <- Ex

#set variables for file import & processing
#DataPathMD <- file.path("RawData", "TestRaw", fsep = .Platform$file.sep)
#DataOut <- "ProcessData"

#Read in from OneDrive; file path is fussy and contains spaces & " - Mount Allison University"
#Remember to force sync to local harddrive b/f attempting import into RStudio

#Consider whether to read in and process each day separately, or use recursive read in to accumulate files from all days together, then filter upon output?

DataInOneDrive <- "~/OneDrive - Mount Allison University/BIOL2201_2024/WI2024Labs/LAB3_MicroGrowth/TUESDAY_GrowthData"

DataOutOneDrive <- "~/OneDrive - Mount Allison University/BIOL2201_2024/WI2024Labs/LAB3_MicroGrowth/TUES_StudentData"

file_id <- ".txt"


FileEncodeMD <- "UTF-16LE" 
DelimiterMD <- "\t"
HeaderRowsMD <- 2

#Temporarily move data files to ~/Documents/GitHub/DougTeach/
#list of files
ExEm_files <- list.files(path = DataInOneDrive, pattern = file_id, full.names = TRUE)

ExEm_files
#test for duplicate file names in chl_files
unique(duplicated(ExEm_files))

MetaData <- readxl::read_excel(file.path("~/OneDrive - Mount Allison University/BIOL2201_2024/WI2024Labs/LAB3_MicroGrowth/WellPlateMetaData.xlsx"), sheet = LabDay)


#MetaData <- file.path("OneDrive - Mount Allison University/BIOL2201_2024/BIOL2201LabMaterials/MicrobialGrowth/WellPlateMetaDataTest.xlsx")

#MetaData <- "https://docs.google.com/spreadsheets/d/1FTKwYlJXd0ze9WwVXCvtwe5qkpN3_HNbeeM3hg7xnrI/edit#gid=0"

```


```{r guess encoding}
guess_encoding(file = ExEm_files[1])

```

```{r read ExEm files}
#"id" parameter of read_delim might replace read_delim_plus
#a read function using tidyverse::read_delim that skips a fixed number of header rows, and adds columns to the dataframe containing the filename and the file creation date time.
# 
read_delim_plus <- function(flnm, delimiter, headerrows, fileencode){read_delim(flnm, delim = delimiter,  col_names = TRUE,  skip = headerrows, escape_double = FALSE,  locale = locale(encoding = fileencode), trim_ws = TRUE) |>
    mutate(Filename = flnm)
  }

ExEm_data <- ExEm_files |>
  map_df(~read_delim_plus(flnm = ., delimiter = DelimiterMD,  headerrows = HeaderRowsMD,  fileencode = FileEncodeMD))

```


```{r tidy ExEm_data}
ExEm_data2 <- ExEm_data |>
  select(-c("Temperature(Â¡C)", "...27")) |>
  separate(Filename, into = c("Path1", "Path2","Path3", "Path4", "Path5", "Path6", "Path7", "Path8","Path9", "Path10", "Path11", "YYYYMMDD", "HHMM","DAY", "LIGHT", "txt"), sep = "([\\/\\_\\.])", remove = FALSE) |>
 select(-c("Path1", "Path2","Path3", "Path4", "Path5", "Path6", "Path7", "Path8","Path9", "Path10", "Path11",  "txt")) |>
  rename(MeasurePt = `...1`)

#smarter way to code this I am sure
 ExEm_data3 <-  ExEm_data2 |>
   group_by(Filename) |>
   mutate(Ex_nm = case_when(row_number() %in% c(1:5) ~ Ex[1],
                            row_number() %in% c(6:10) ~ Ex[2],
                            row_number() %in% c(11:15)  ~ Ex[3]), .after = MeasurePt) |>
  relocate(c("YYYYMMDD", "HHMM","DAY", "LIGHT")) |>
  unite(YYYYMMDD_HHMM,  YYYYMMDD:HHMM, sep = "_", remove = FALSE) |>
  mutate(YYYYMMDD_HHMM = ymd_hm(YYYYMMDD_HHMM), 
         Ex_nm = as.numeric(Ex_nm)) |>
  filter(!is.na(Ex_nm))
  
```

Tricky file format
Ex_nm, Em_nm missing from file format
5 measure points x 3 Ex_nm for each well

Check to make sure E_hours working when more files accumulate
```{r E_hours}
ExEm_data3 <- ExEm_data3 |>
  group_by(DAY, LIGHT) |>
  mutate(E_hours =  as.numeric((YYYYMMDD_HHMM - min(YYYYMMDD_HHMM))) / 3600, .after = YYYYMMDD_HHMM) |>
  ungroup()
```

```{r summarize 5 MeasurePts for each Ex_nm and file}

ExEm_mean <- ExEm_data3 %>%
  group_by(YYYYMMDD_HHMM, E_hours, YYYYMMDD, HHMM, DAY, LIGHT, Ex_nm, Filename) %>%
  summarize(across(A1:D6, mean)) %>%
  ungroup()
  
```

Export wide format OD_data to OneDrive for Excel Analyses
With Just 'HIGH' and 'LOW' plates for each DAY in separate folders, may export data for each DAY simply by re-running script.

```{r export DAY csv}

ExEm_mean |>
  filter(Ex_nm == 440) |> #only include Chl data
  #write_csv(file = file.path(DataOutOneDrive, "test.csv"))
  write_csv(file = file.path(DataOutOneDrive, paste(Project, LabDay, "Chl", ".csv", sep = "_")))

```


More complicated version needed previously.
Need to use old %>% magrittr::pipe for compatibility; base R |> does not work
Fix this once data folders accumulate; Day, Bench, Light
https://www.tidyverse.org/blog/2023/05/purrr-walk-this-way/

```{r}
# OD_data %>%
#   filter(Wavelength_nm == 600) %>%
#   unite("Group_Bench", Group, Bench, sep = "_") %>%
#   arrange(Temp_C) %>%
#   nest(.by = c(Group_Bench)) %>%
#   {walk2(.x = .$data, .y = .$Group_Bench, ~ write_csv(.x, file = file.path("..", "..", "..", "..", DataOneDrive, str_c(.y, ".csv"))))}

```


```{r long format}
ExEm_long <- ExEm_mean |>
  pivot_longer(cols = -c(YYYYMMDD_HHMM:Ex_nm, Filename), names_to = "WELL", values_to = "OD") |>
  separate(WELL, into = c("ROW", "COL"), sep = 1, remove = FALSE)

```

Need to Fix MetaData variable columns and File data columns for left join.
Need DAY, LIGHT, WELL in both MetaData and File data
```{r merge with MetaData}
ExEm_meta <- left_join(ExEm_long, MetaData, by = join_by(DAY, LIGHT, WELL))


```


```{r save OD600Meta, echo=FALSE}
saveRDS(ODMeta, file.path(DataOut, 
paste(Project, "ODMeta.Rds", sep = "_"), fsep = .Platform$file.sep))

```

#https://stackoverflow.com/questions/72906809/read-and-write-excel-and-csv-files-from-sharepoint-via-r
#need to get metadata working to implement this export; should work.
```{r export OD600.csv to shared OneDrive folder}
#OD600Meta |>
  # #select(-c(Media_mL, Innoc_mL, Row, Col, logOD_600)) |>
  # pivot_wider(names_from = c(Well, Wavelength_nm, MysteryID), values_from = c(OD)) |>
   #pivot_wider(names_from = c(Well, Wavelength_nm), values_from = c(OD)) |>
  # arrange(Temp_C) |>
  # unite("Group_Bench", Group, Bench, sep = "_") |>
  # nest(.by = c(Group_Bench)) |>
  # {walk2(.x = .$data, .y = .$Group_Bench, ~ write_csv(.x, file = file.path("..", "..", "..", "..", DataOneDrive, str_c(.y, ".csv"))))}

```

XXXX 23 JAN 2024 XXXX

Implement MultiSpectral Analyses and Growth Curve fits well by well.
Define equations as functions.
x will be taken from 'E_days' when we run the fit.
```{r logistic_eqn}
LogisticEqn <-  function(x, Pmax, Mu, Intercept){(Pmax*Intercept*exp(Mu*x))/(Pmax + (Intercept*(exp(Mu*x)-1)))
}

ModGompertzEqn <- function(x, Gmax,Gmu,Lag){(Gmax*(exp(-exp((Gmu*exp(1))/Gmax*(Lag-x)+1))))}

possibnlSLM = possibly(.f = nlsLM, otherwise = NULL)
```

Fit treatment specific logistic growth trajectories using nest purrr:map & broom::augment using nest specific start, lower & upper settings extracted from each nest on the fly.
This may be necessary if the 'nests' contain diverse data patterns that fail to fit with generic start, lower & upper parameters extracted from the entire dataset

```{r growth fits}
OD_nest <- ODMeta |>
  nest(data = c(YYYYMMDD_HHMM, Filename, E_hours, OD))

OD_nest <- OD_nest |> 
  mutate(FitLog = map(data, ~possibnlSLM(OD ~ LogisticEqn(x = E_hours, Intercept, Mu, Pmax),
                                         data = .x,
                                         start = list(Intercept = min(.$OD, na.rm = TRUE), Mu = 0.1, Pmax = max(.$OD, na.rm = TRUE))
                                         )
  ),
  PredictLog = map(FitLog, augment),
  TidiedLog = map(FitLog, tidy),
  ParamLog = map(FitLog, glance)
  )

OD_nest |>
  unnest(PredictLog) |>
  ggplot() + 
  #geom_point(aes(x = E_hours, y = OD)) +
  geom_point(aes(x = E_hours, y = OD, colour = as.factor(Wavelength_nm))) + 
  geom_ribbon(aes(x = E_hours, ymin =  `.fitted` - `.resid`, ymax =  `.fitted` + `.resid`, colour = as.factor(Wavelength_nm))) +
  scale_colour_manual(values = ColourNM) +
  facet_grid(row = vars(Row), col = vars(Col)) +
  theme_bw()

OD_nest |> 
  dplyr::select(-c(data, FitLog, PredictLog, ParamLog)) |>
  unnest(TidiedLog) |>
  select(-statistic) |>
  pivot_wider(names_from = term, values_from = c(estimate:p.value))

OD_nest |> 
  dplyr::select(-c(data, FitLog, PredictLog, ParamLog)) |>
  unnest(TidiedLog) |>
  select(-statistic) |>
  pivot_wider(names_from = term, values_from = c(estimate:p.value)) |>
  filter(Well == "A1") |>
  ggplot() +
  geom_point(aes(x = Wavelength_nm, y = estimate_Mu)) +
  theme_bw()

```

