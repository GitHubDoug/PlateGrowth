---
title: "Import microbial plate growth data"
author: "Maximilian Berthold, Douglas A. Campbell"
date: "`r format(Sys.Date())`"
output:
  html_document:
    df_print: paged
    code_folding: hide
---

This .Rmd imports Molecular Device Fluorescence well plate files, generated using multiple excitation/emission wavelength pairs.
The size of the well plate does not matter; the import creates columns for RowColumn LetterNumber (A01 etc.) for the largest plate type in the list of files.
Import of smaller plate types results in NA in the non-existent RowColumn for that plate.


```{r load libraries, echo=FALSE, message = FALSE, warning = FALSE} 
# libraries; Note check actual dependencies
library(tidyverse) #core tidyverse packages
library(minpack.lm) #curve fitting
library(broom) #tidy model outputs
library(photobiology) #photobiology::w_length2rgb

```

Fix File Paths for Import once DataSets accumulate
```{r variable names for file import & processing MOLECULAR DEVICES}
Project <- "BIOL2201"
LabDay <- "TUES"

#File Ex Em nm settings lost from files during import b/c of Header structure

Em <- 680

Ex <- c(440, 520, 620)

#Add colour coding vector
ExNM = (photobiology::w_length2rgb(Ex))
names(ExNM) <- Ex

#set variables for file import & processing
#Read in from OneDrive; file path is fussy and contains spaces & " - Mount Allison University"
#Remember to force sync to local harddrive b/f attempting import into RStudio

#Currently read in and process each LabDay separately; Alternative could use recursive read in to accumulate files from all LabDays together, then filter upon output?

DataInOneDrive <- paste("~/OneDrive - Mount Allison University/BIOL2201_2024/WI2024Labs/LAB3_MicroGrowth/", LabDay, "_GrowthData", sep = "")

DataOutOneDrive <- paste("~/OneDrive - Mount Allison University/BIOL2201_2024/WI2024Labs/LAB3_MicroGrowth/", LabDay, "_StudentData", sep = "")

file_id <- ".txt"

FileEncodeMD <- "UTF-16LE" 
DelimiterMD <- "\t"
HeaderRowsMD <- 2

#list of files
ExEm_files <- list.files(path = DataInOneDrive, pattern = file_id, full.names = TRUE)

ExEm_files
#test for duplicate file names in chl_files
unique(duplicated(ExEm_files))

#MetaData <- readxl::read_excel(file.path("~/OneDrive - Mount Allison University/BIOL2201_2024/WI2024Labs/LAB3_MicroGrowth/WellPlateMetaData.xlsx"), sheet = LabDay)

#Local OneDrive MetaData path does not work on Posit.cloud
#MetaData, moved from OneDrive to git tracked project, to github, update with 'pull' from GitHub
MetaData <- readxl::read_excel(file.path("WellPlateMetaData.xlsx"), sheet = LabDay)

```


```{r guess encoding}
guess_encoding(file = ExEm_files[1])

```

```{r read ExEm files}
#"id" parameter of read_delim might replace read_delim_plus
#a read function using tidyverse::read_delim that skips a fixed number of header rows, and adds columns to the dataframe containing the filename and the file creation date time.
# 
read_delim_plus <- function(flnm, delimiter, headerrows, fileencode){read_delim(flnm, delim = delimiter,  col_names = TRUE,  skip = headerrows, escape_double = FALSE,  locale = locale(encoding = fileencode), trim_ws = TRUE) |>
    mutate(Filename = flnm)
  }

ExEm_data <- ExEm_files |>
  map_df(~read_delim_plus(flnm = ., delimiter = DelimiterMD,  headerrows = HeaderRowsMD,  fileencode = FileEncodeMD))

```


```{r tidy ExEm_data}
ExEm_data2 <- ExEm_data |>
  select(-c("Temperature(Â¡C)", "...27")) |>
  separate(Filename, into = c("Path1", "Path2","Path3", "Path4", "Path5", "Path6", "Path7", "Path8","Path9", "Path10", "Path11", "YYYYMMDD", "HHMM","DAY", "LIGHT", "txt"), sep = "([\\/\\_\\.])", remove = FALSE) |>
 select(-c("Path1", "Path2","Path3", "Path4", "Path5", "Path6", "Path7", "Path8","Path9", "Path10", "Path11",  "txt")) |>
  rename(MeasurePt = `...1`)

#smarter way to code this I am sure
 ExEm_data3 <-  ExEm_data2 |>
   group_by(Filename) |>
   mutate(Ex_nm = case_when(row_number() %in% c(1:5) ~ Ex[1],
                            row_number() %in% c(6:10) ~ Ex[2],
                            row_number() %in% c(11:15)  ~ Ex[3]), .after = MeasurePt) |>
  relocate(c("YYYYMMDD", "HHMM","DAY", "LIGHT")) |>
  unite(YYYYMMDD_HHMM,  YYYYMMDD:HHMM, sep = "_", remove = FALSE) |>
  mutate(YYYYMMDD_HHMM = ymd_hm(YYYYMMDD_HHMM), 
         Ex_nm = as.numeric(Ex_nm)) |>
  filter(!is.na(Ex_nm))
  
```

Tricky file format
Ex_nm, Em_nm missing from file format
5 measure points x 3 Ex_nm for each well

Check to make sure E_hours working when more files accumulate
```{r E_hours}
ExEm_data3 <- ExEm_data3 |>
  group_by(DAY, LIGHT) |>
  mutate(E_hours =  as.numeric((YYYYMMDD_HHMM - min(YYYYMMDD_HHMM))) / 3600, .after = YYYYMMDD_HHMM) |>
  ungroup()
```

```{r summarize 5 MeasurePts for each Ex_nm and file}

ExEm_mean <- ExEm_data3 %>%
  group_by(YYYYMMDD_HHMM, E_hours, YYYYMMDD, HHMM, DAY, LIGHT, Ex_nm, Filename) %>%
  summarize(across(A1:D6, mean)) %>%
  ungroup()

```

Export wide format OD_data to OneDrive for Excel Analyses
With Just 'HIGH' and 'LOW' plates for each DAY in separate folders, may export data for each DAY simply by re-running script.

```{r export DAY csv}

ExEm_mean |>
  filter(Ex_nm == 440) |> #only include Chl data
  arrange(LIGHT, E_hours) |>#order rows by Light and by E_hours
  write_csv(file = file.path(DataOutOneDrive, paste(Project, LabDay, "Chl", ".csv", sep = "_")))

```


More complicated version needed previously.
Need to use old %>% magrittr::pipe for compatibility; base R |> does not work
Fix this once data folders accumulate; Day, Bench, Light
https://www.tidyverse.org/blog/2023/05/purrr-walk-this-way/

```{r}
# OD_data %>%
#   filter(Wavelength_nm == 600) %>%
#   unite("Group_Bench", Group, Bench, sep = "_") %>%
#   arrange(Temp_C) %>%
#   nest(.by = c(Group_Bench)) %>%
#   {walk2(.x = .$data, .y = .$Group_Bench, ~ write_csv(.x, file = file.path("..", "..", "..", "..", DataOneDrive, str_c(.y, ".csv"))))}

```


```{r long format}
ExEm_long <- ExEm_mean |>
  pivot_longer(cols = -c(YYYYMMDD_HHMM:Ex_nm, Filename), names_to = "WELL", values_to = "Fluor") |>
  separate(WELL, into = c("Row", "Col"), sep = 1, remove = FALSE)

ExEm_long |>
  filter(Ex_nm == 440) |>
  ggplot() +
  geom_point(aes(x = E_hours, y = Fluor, colour = LIGHT)) +
  facet_grid(rows = vars(`Row`), cols = vars(`Col`)) + 
  scale_y_continuous(sec.axis = sec_axis(~ . , name = sym("Row"), breaks = NULL, labels = NULL)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = sym("Col"), breaks = NULL, labels = NULL)) +
  labs(title = LabDay, subtitle = paste(Ex[1], "_nm", sep = "")) +
  theme_bw()

#ggsave(filename = file.path("Plots", "WellPlateFluorExample.png"))
```

Need to Fix MetaData variable columns and File data columns for left join.
Need DAY, LIGHT, WELL in both MetaData and File data
```{r merge with MetaData}
ExEm_meta <- left_join(ExEm_long, MetaData, by = join_by(DAY, LIGHT, WELL)) |>
  filter(!is.na(SAMPLE))

```

```{r test plot}
ExEm_meta |>
  filter(Ex_nm == 440) |>
  filter(Fluor >= 0,
         Fluor <= 1000) |> #hack fix to remove bad points from 620 nm Ex, 20 h)
  ggplot() +
  geom_point(aes(x = E_hours, y = Fluor, colour = INITIAL)) +
  #scale_colour_manual(values = ExNM) +
  facet_grid(rows = vars(SAMPLE), cols = vars(LIGHT)) +
  labs(title = LabDay, subtitle = paste(Ex[1], "_nm"))+
  theme_bw()

ExEm_meta |>
   filter(Fluor >= 0,
          Fluor <= 1000) |> #hack fix to remove bad points from 620 nm Ex, 20 h)
  ggplot() +
  geom_point(aes(x = E_hours, y = Fluor, colour = LIGHT)) +
  #scale_colour_manual(values = ExNM) +
  facet_grid(rows = vars(SAMPLE), cols = vars(Ex_nm)) +
  labs(title = LabDay) +
  theme_bw()
```

```{r save ExEm_meta, echo=FALSE}

ExEm_meta |>
  filter(Fluor >= 0,
         Fluor <= 500) |> #hack fix to remove bad points from 620 nm Ex, 20 h)
  saveRDS(file.path(DataOutOneDrive, paste(Project, LabDay, "ExEm_meta.Rds", sep = "_"), fsep = .Platform$file.sep))

```

#https://stackoverflow.com/questions/72906809/read-and-write-excel-and-csv-files-from-sharepoint-via-r

```{r export OD600.csv to shared OneDrive folder}
#OD600Meta |>
  # #select(-c(Media_mL, Innoc_mL, Row, Col, logOD_600)) |>
  # pivot_wider(names_from = c(Well, Wavelength_nm, MysteryID), values_from = c(OD)) |>
   #pivot_wider(names_from = c(Well, Wavelength_nm), values_from = c(OD)) |>
  # arrange(Temp_C) |>
  # unite("Group_Bench", Group, Bench, sep = "_") |>
  # nest(.by = c(Group_Bench)) |>
  # {walk2(.x = .$data, .y = .$Group_Bench, ~ write_csv(.x, file = file.path("..", "..", "..", "..", DataOneDrive, str_c(.y, ".csv"))))}

```

Implement MultiSpectral Analyses and Growth Curve fits well by well.
Define equations as functions.
x will be taken from 'E_days' when we run the fit.
```{r logistic_eqn}
LogisticEqn <-  function(x, Pmax, Mu, Intercept){(Pmax*Intercept*exp(Mu*x))/(Pmax + (Intercept*(exp(Mu*x)-1)))
}

#ModGompertzEqn <- function(x, Gmax,Gmu,Lag){(Gmax*(exp(-exp((Gmu*exp(1))/Gmax*(Lag-x)+1))))}

possibnlSLM = possibly(.f = nlsLM, otherwise = NULL)
```

Fit treatment specific logistic growth trajectories using nest purrr:map & broom::augment using nest specific start, lower & upper settings extracted from each nest on the fly.
This may be necessary if the 'nests' contain diverse data patterns that fail to fit with generic start, lower & upper parameters extracted from the entire dataset

```{r growth fits}
ExEm_nest <- ExEm_meta |>
  filter(Fluor >= 0,
         Fluor <= 500) |> #hack fix to remove bad points from 620 nm Ex, 20 h)
  nest(data = -c(DAY, LIGHT, Ex_nm, SAMPLE, PAR_uE, Temp_C))

ExEm_nest <- ExEm_nest|> 
  mutate(FitLog = map(data, ~possibnlSLM(Fluor ~ LogisticEqn(x = E_hours, Intercept, Mu, Pmax),
                                         data = .x,
                                         start = list(Intercept = min(.$Fluor, na.rm = TRUE), Mu = 0.1, Pmax = max(.$Fluor, na.rm = TRUE))
                                         )
  ),
  PredictLog = map(FitLog, augment),
  TidiedLog = map(FitLog, tidy),
  ParamLog = map(FitLog, glance)
  )

ExEm_nest |>
  unnest(PredictLog) |>
  ggplot() + 
  geom_point(aes(x = E_hours, y = Fluor, colour = as.factor(Ex_nm))) + 
  geom_line(aes(x = E_hours, y = `.fitted`, colour = as.factor(Ex_nm))) +
  geom_point(aes(x = E_hours, y =  `.resid`), size = 0.1) +
  scale_colour_manual(values = ExNM) +
  facet_grid(row = vars(SAMPLE), col = vars(LIGHT)) +
  theme_bw()

ExEm_nest |> 
  dplyr::select(-c(data, FitLog, PredictLog, ParamLog)) |>
  unnest(TidiedLog) |>
  select(-statistic) |>
  pivot_wider(names_from = term, values_from = c(estimate:p.value))

ExEm_nest |> 
  dplyr::select(-c(data, FitLog, PredictLog, ParamLog)) |>
  unnest(TidiedLog) |>
  select(-statistic) |>
  pivot_wider(names_from = term, values_from = c(estimate:p.value)) |>
  ggplot() +
  geom_point(aes(x = Ex_nm, y = estimate_Mu, colour = LIGHT)) +
  facet_grid(rows = vars(SAMPLE)) +
  theme_bw()

ExEm_nest |> 
  dplyr::select(-c(data, FitLog, PredictLog, ParamLog)) |>
  unnest(TidiedLog) |>
  select(-statistic) |>
  pivot_wider(names_from = term, values_from = c(estimate:p.value)) |>
  ggplot() +
  geom_point(aes(x = Ex_nm, y = estimate_Pmax, colour = LIGHT)) +
  facet_grid(rows = vars(SAMPLE)) +
  theme_bw()

```

