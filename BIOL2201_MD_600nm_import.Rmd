---
title: "Import and fit microbial plate growth data"
author: "Maximilian Berthold, Douglas A. Campbell"
date: "`r format(Sys.Date())`"
output:
  html_document:
    df_print: paged
    code_folding: hide
---

This .Rmd imports Molecular Device Fluorescence well plate files, with data from one plates in the file.
The size of the well plate does not matter; the import creates columns for RowColumn LetterNumber (A01 etc.) for the largest plate type in the list of files.
Import of smaller plate types results in NA in the non-existent RowColumn for that plate.

```{r load libraries, echo=FALSE, message = FALSE, warning = FALSE} 
# libraries; Note check actual dependencies
library(tidyverse) #core tidyverse packages
library(lubridate) #tidyverse dates
library(googledrive) #accessing googledrive
library(googlesheets4) #accessing googlesheets
library(minpack.lm) #curve fitting
library(broom) #tidy model outputs

```

```{r variable names for file import & processing MOLECULAR DEVICES}
Project <- "BIOL2201"
DayGroup = "THURS"
DayBench = "B1"


#set variables for file import & processing
DataPath <- file.path("RawData", "Single600Files", fsep = .Platform$file.sep)
file_id <- ".txt"

DataOut <- "ProcessData"
DataOneDrive <- "OneDrive - Mount Allison University/BIOL2201_2024/BIOL2201LabMaterials/MicrobialGrowth/PlateData"

FileEncodeMD <- "UTF-16LE" 
DelimiterMD <- "\t"
HeaderRowsMD <- 2


MetaData <- read_csv(file.path("OneDrive - Mount Allison University/BIOL2201_2024/BIOL2201LabMaterials/MicrobialGrowth/WellPlateMetaDataTest.xlsx"))

OD_files <- list.files(path = DataPath, pattern = file_id, full.names = TRUE)

#MetaData <- file.path("OneDrive - Mount Allison University/BIOL2201_2024/BIOL2201LabMaterials/MicrobialGrowth/WellPlateMetaDataTest.xlsx")

#MetaData <- "https://docs.google.com/spreadsheets/d/1FTKwYlJXd0ze9WwVXCvtwe5qkpN3_HNbeeM3hg7xnrI/edit#gid=0"
```

Load MetaData from OneDrive
```{r read metadata}
#deauthorizes access to googlesheet
#gs4_deauth()

#read contents of MetaData from GoogleSheet

# MetaData20 <- read_sheet(MetaDataURL, sheet = "20")
# MetaData37 <- read_sheet(MetaDataURL, sheet = "37")
# MetaData <- rbind(MetaData20, MetaData37)
# 
# rm(MetaData20, MetaData37)

#correct values and variable names in MetaData
# MetaData <- MetaData %>%
#   mutate(Group = case_when(
#     Day =="TUESDAY" ~"TUES",
#     Day =="WEDNESDAY" ~"WED",
#     Day == "THURSDAY" ~ "THURS"
#   ), .after = Day) %>%
#   mutate(MysteryID = as.factor(as.character(MysteryID))) %>%
#   select(-c("Initials"))

MetaData <- read_csv(file = MetaData)
```


```{r list data files}
OD_files <- list.files(path = DataPath, pattern = file_id, full.names = TRUE)

OD_files
#test for duplicate file names in chl_files
unique(duplicated(OD_files))

# guess encoding
guess_encoding(file = OD_files[1])
```

```{r read files, warning = FALSE, message = FALSE}
#"id" parameter of read_delim might replace read_delim_plus
#a read function using tidyverse::read_delim that skips a fixed number of header rows, and adds columns to the dataframe containing the filename and the file creation date time.
# 
read_delim_plus <- function(flnm, delimiter, headerrows, fileencode){read_delim(flnm, delim = delimiter,  col_names = TRUE,  skip = headerrows, escape_double = FALSE,  locale = locale(encoding = fileencode), trim_ws = TRUE) %>%
    mutate(Filename = flnm)
  }


OD_data <- OD_files %>%
  map_df(~read_delim_plus(flnm = ., delimiter = DelimiterMD,  headerrows = HeaderRowsMD,  fileencode = FileEncodeMD))

```

```{r tidy OD_data}
OD <- OD_data %>%
  select(-c("...1", "Temperature(Â¡C)", "...27")) %>%
  filter(!is.na(A1))  %>%
  separate(Filename, into = c("Path1", "Path2", "DDMMYYYY", "HHMM","Group", "Bench", "Temp_C", "txt"), sep = "([\\/\\_\\.])", remove = FALSE) %>%
  select(-c("Path1", "Path2", "txt")) %>%
  relocate(c("DDMMYYYY", "HHMM","Group", "Bench", "Temp_C")) %>%
  unite(DDMMYYYY_HHMM,  DDMMYYYY:HHMM, sep = "_", remove = TRUE) %>%
  mutate(DDMMYYYY_HHMM = dmy_hm(DDMMYYYY_HHMM)) %>%
  mutate(Temp_C = as.numeric(Temp_C))

#3 rows of data to average from each well measurement

OD <- OD %>%
  group_by(DDMMYYYY_HHMM, Group, Bench, Temp_C, Filename) %>%
  summarize(across(A1:D6, mean)) %>%
  ungroup()
  
```


```{r E_hours}
OD <- OD %>%
  group_by(Group, Bench, Temp_C) %>%
  mutate(E_hours =  as.numeric((DDMMYYYY_HHMM - min(DDMMYYYY_HHMM))) / 3600, .after = DDMMYYYY_HHMM) %>%
  ungroup()
```

```{r OD test plot}
OD %>% ggplot() +
  geom_point(aes(x = E_hours, y = A1)) + 
  facet_grid(rows = vars(Group), cols = vars(Bench, Temp_C)) +
   scale_x_continuous(sec.axis = sec_axis(~ . , name = sym("Bench"), breaks = NULL, labels = NULL)) +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = sym("Group"), breaks = NULL, labels = NULL)) +
  theme_bw()

OD %>% ggplot() +
  geom_point(aes(x = E_hours, y = A1)) + 
  facet_grid(rows = vars(Group), cols = vars(Bench, Temp_C)) +
   scale_x_continuous(sec.axis = sec_axis(~ . , name = sym("Bench"), breaks = NULL, labels = NULL)) +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = sym("Group"), breaks = NULL, labels = NULL)) +
  theme_bw()

```



```{r long format}
OD_long <- OD %>%
  pivot_longer(cols = -c(DDMMYYYY_HHMM, E_hours, Group, Bench, Temp_C, Filename), names_to = "Well", values_to = "OD_600") %>%
  separate(Well, into = c("Row", "Col"), sep = 1, remove = FALSE)

```


```{r logOD}
OD_long <- OD_long %>%
  mutate(logOD_600 = log(OD_600))
```

```{r OD facet plot}
OD_long %>% 
  filter(Temp_C == 20,
         Group == DayGroup) %>%
  ggplot() +
  geom_point((aes(x = E_hours, y = logOD_600, colour = Row))) + 
  facet_grid(rows = vars(Bench), cols = vars(Col)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Column", breaks = NULL, labels = NULL)) +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = sym("Bench"), breaks = NULL, labels = NULL)) +
  labs(title = DayGroup) +
  theme_bw()

OD_long %>% 
  filter(Temp_C == 20,
         Group == DayGroup,
         Bench == "B1") %>%
  ggplot() +
  geom_point((aes(x = E_hours, y = OD_600))) + 
  facet_grid(rows = vars(Row), cols = vars(Col)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = " Plate Column", breaks = NULL, labels = NULL)) +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = sym("Plate Row"), breaks = NULL, labels = NULL)) +
  theme_bw()

ggsave(filename = file.path("Plots", "WellPlateExample.png") )
```

Merge OD_long with MetaData
```{r left_join OD_long with MetaData}

OD_meta <- left_join(x = OD_long, y = MetaData, by = c("Group", "Bench", "Temp_C", "Well"), keep = FALSE)

```

```{r filter out blank data rows}
OD_meta <- OD_meta %>%
  filter(MysteryID != "TSB",
         MysteryID != "BLANK")
```

```{r save OD_meta, echo=FALSE}
saveRDS(OD_meta, file.path(DataOut, 
paste(Project, "OD_meta.Rds", sep = "_"), fsep = .Platform$file.sep))
```

```{r export bench specific csv}
DayGroup = "THURS"
DayBench = "B1"

OD_meta %>%
  filter(Group == DayGroup) %>%
  filter(Bench == DayBench) %>%
  select(-c(Day, Media_mL, Innoc_mL, Row, Col, logOD_600)) %>%
  pivot_wider(names_from = c(Well, MysteryID), values_from = c(OD_600)) %>%
  arrange(Temp_C) %>%
  write_csv(., file = file.path("ProcessData", paste(Project, "_", DayGroup,"_", DayBench, "_OD600", ".csv", sep = "")))

  #bulk write of nested dataframe to .csv, instead of filtering
  #https://community.rstudio.com/t/odd-behavior-with-walk2-and-pipe/5543/5
#need to use %>% pipe, not |> pipe

#walk2 works, pwalk does not, so create GroupBench variable for grouping, instead of grouping by Group and Bench (preferred)    
  OD_meta %>%
  select(-c(Media_mL, Innoc_mL, Row, Col, logOD_600)) %>%
  pivot_wider(names_from = c(Well, MysteryID), values_from = c(OD_600)) %>%
  arrange(Temp_C) %>%
  unite("Group_Bench", Group, Bench, sep = "_") %>%
  nest(.by = c(Group_Bench)) %>%
  {walk2(.x = .$data, .y = .$Group_Bench, ~ write_csv(.x, file = file.path(DataOut, str_c(.y, ".csv"))))}


 # walk2(.x = test$data, .y = test$Group, ~ write_csv(.x, file = file.path(DataOut, str_c(.y, ".csv"))))

#  https://stackoverflow.com/questions/49265737/applying-purrrwalk2-to-a-data-frame-of-data-frames-at-the-end-of-a-pipe
  
  # test2 <- OD_meta %>%
  # select(-c(Media_mL, Innoc_mL, Row, Col, logOD_600)) %>%
  # pivot_wider(names_from = c(Well, MysteryID), values_from = c(OD_600)) %>%
  # arrange(Temp_C) %>%
  # nest(.by = c(Group, Bench))
  # 
  # # %>%
  # # {pwalk(list(.$data, .$Group, .$Bench), ~ write_csv(.$data, file = paste0(.$Group, .$Bench, ".csv")))}
  # 
  # pwalk(list(data = test2$data, group = test2$Group, bench = test2$Bench), ~ write_csv(data, file = str_c(group, bench, ".csv"))) 
```

```{r export GroupBench data to googlesheets}
# TestSheet <- OD_meta %>%
#   filter(Group == DayGroup) %>%
#   filter(Bench == DayBench) %>%
#   select(-c(Day, Media_mL, Innoc_mL, Row, Col, logOD_600)) %>%
#   pivot_wider(names_from = c(Well, MysteryID), values_from = c(OD_600)) %>%
#   arrange(Temp_C) %>%
#   write_csv(., file = file.path("ProcessData", paste(Project, "_", DayGroup,"_", DayBench, "_OD600", ".csv", sep = "")))

# gs4_auth()
# gs4_create("BIOL2201_2024_GrowthData")
# id <- googledrive::drive_get("BIOL2201_2024_GrowthData")

#https://docs.google.com/spreadsheets/d/1_Rvfe3UV0ph6dmsu-H9aG-_l9a90wQNthB-JJCqmWDE/edit#gid=0

# sheet_write(TestSheet, ss = "https://docs.google.com/spreadsheets/d/1_Rvfe3UV0ph6dmsu-H9aG-_l9a90wQNthB-JJCqmWDE/edit#gid=0", sheet = "TestSheet")

#Starts but fails; maybe filter out superfluous content b/f write to GoogleSheets; trying to send data to Googlesheets faster than write quota

# https://stackoverflow.com/questions/66186332/googlesheets-quota-limit-issues-possible-failure-to-use-api-key


 # OD_meta %>%
 #  select(-c(Media_mL, Innoc_mL, Row, Col, logOD_600)) %>%
 #  pivot_wider(names_from = c(Well, MysteryID), values_from = c(OD_600)) %>%
 #  arrange(Temp_C) %>%
 #  unite("Group_Bench", Group, Bench, sep = "_") %>%
 #  nest(.by = c(Group_Bench)) %>%
 #   {walk2(.x = .$data, .y = .$Group_Bench, ~ sheet_write(.x, ss = "https://docs.google.com/spreadsheets/d/1_Rvfe3UV0ph6dmsu-H9aG-_l9a90wQNthB-JJCqmWDE/edit#gid=0", sheet = .y))}
 
# Show in New Window
# â Writing to BIOL2201_GrowthData_2024.
# â Writing to sheet TUES_B3.
# Error in `map2()`:
# â¹ In index: 1.
# Caused by error in `response_as_json()`:
# ! Expected content type application/json, not text/html.
# â¢ <!DOCTYPE html> <html lang=en> <meta charset=utf-8> <meta name=viewport
#   content="initial-scale=1, minimum-scale=1, width=device-width"> <title>Error 408 (Request
#   Timeout)!!1</title> <style>...
# Backtrace:
#   1. ... %>% ...
#   2. purrr::walk2(...)
#   3. purrr::map2(.x, .y, .f, ..., .progress = .progress)
#   4. purrr:::map2_("list", .x, .y, .f, ..., .progress = .progress)
#   8. .f(.x[[i]], .y[[i]], ...)
#      ...
#  11. gargle::request_retry(x, ..., encode = encode, user_agent = gs4_user_agent())
#  12. gargle:::backoff(tries_made, resp, base = b)
#  13. gargle:::sheets_per_user_quota_exhaustion(resp)
#  14. gargle::gargle_error_message(resp)
#  15. gargle::response_as_json(resp)
#  
```
#https://stackoverflow.com/questions/72906809/read-and-write-excel-and-csv-files-from-sharepoint-via-r

```{r export .csv to shared OneDrive folder}
  OD_meta %>%
  select(-c(Media_mL, Innoc_mL, Row, Col, logOD_600)) %>%
  pivot_wider(names_from = c(Well, MysteryID), values_from = c(OD_600)) %>%
  arrange(Temp_C) %>%
  unite("Group_Bench", Group, Bench, sep = "_") %>%
  nest(.by = c(Group_Bench)) %>%
  {walk2(.x = .$data, .y = .$Group_Bench, ~ write_csv(.x, file = file.path("..", "..", "..", "..", DataOneDrive, str_c(.y, ".csv"))))}

```

Implement Growth Curve fits well by well.
Define equations as functions.
x will be taken from 'E_days' when we run the fit.
```{r logistic_eqn}
LogisticEqn <-  function(x, Pmax, Mu, Intercept){(Pmax*Intercept*exp(Mu*x))/(Pmax + (Intercept*(exp(Mu*x)-1)))
}

possibnlSLM = possibly(.f = nlsLM, otherwise = NULL)
```

Fit logistic growth trajectories using nest purrr:map & broom::augment using start, lower & upper settings extracted from each nest on the fly.
Extracting nest-specific start, lower & upper settings from each nest on the fly may be necessary if the 'nests' contain diverse data patterns that fail to fit with generic start, lower & upper parameters extracted from the entire dataset
```{r growth well fits}
OD_nest <- OD_meta %>%
  filter(Group == DayGroup) %>%
  filter(Bench == DayBench) %>%
  nest(data = c(DDMMYYYY_HHMM, Filename, E_hours, OD_600, logOD_600))

OD_nest <- OD_nest %>%
  mutate(ExpInit_h = as.numeric(map(data, ~((.$logOD_600[2]) - (.$logOD_600[1]))/.$E_hours[2]))) %>%
  mutate(FitLog = map(data, ~possibnlSLM(OD_600 ~ LogisticEqn(x = E_hours, Intercept, Mu, Pmax),
                                         data = .x,
                                         start = list(Intercept = min(.$OD_600, na.rm = TRUE), Mu = 0.1, Pmax = max(.$OD_600, na.rm = TRUE))
                                         )
  ),
  TidiedLog = map(FitLog, tidy),
  PredictLog = map(FitLog, augment)
  )

OD_nest %>%
  unnest(PredictLog) %>%
  ggplot() + 
  geom_point(aes(x = E_hours, y = OD_600)) + 
  geom_line(aes(x = E_hours, y =  `.fitted`)) +
  facet_grid(col = vars(MysteryID, Well), rows = vars(Temp_C)) +
   scale_x_continuous(sec.axis = sec_axis(~ . , name = sym("MysteryID"), breaks = NULL, labels = NULL)) +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = sym("Temp_c"), breaks = NULL, labels = NULL)) +
  labs(title = sym(DayGroup), subtitle = sym(DayBench)) +
  theme_bw()

Mu_est <- OD_nest %>% 
  dplyr::select(-c(data, FitLog, PredictLog)) %>%
  unnest(TidiedLog) %>%
  select(-statistic) %>%
  pivot_wider(names_from = term, values_from = c(estimate:p.value))

Mu_est %>%
  ggplot() +
  geom_point(aes(x = MysteryID, y = estimate_Mu)) +
  geom_errorbar(aes(x = MysteryID, ymin = estimate_Mu - std.error_Mu,  ymax = estimate_Mu + std.error_Mu)) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  facet_grid(rows = vars(Temp_C)) +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = sym("Temp_c"), breaks = NULL, labels = NULL)) +
   labs(title = sym(DayGroup), subtitle = sym(DayBench)) +
  theme_bw()

Mu_est %>%
  ggplot() +
  geom_point(aes(x = MysteryID, y = ExpInit_h)) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  facet_grid(rows = vars(Temp_C)) +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = sym("Temp_c"), breaks = NULL, labels = NULL)) +
   labs(title = sym(DayGroup), subtitle = sym(DayBench)) +
  theme_bw()

Mu_est %>%
  ggplot() + 
  geom_point(aes(x = estimate_Mu, y = ExpInit_h, size = estimate_Pmax)) +
  coord_fixed(ratio = 1, xlim = c(0, 0.2), ylim = c(0, 0.2)) +
  facet_grid(cols = vars(Bench), rows = vars(Temp_C)) +
  labs(title = sym(DayGroup), subtitle = sym(DayBench)) +
  theme_bw()
  
```


Implement Growth Curve fits pool by pool of 4 rows together from one column.

Fit treatment specific logistic growth trajectories using nest purrr:map & broom::augment.
Using nest specific start, lower & upper settings extracted from each nest on the fly. may be necessary if the 'nests' contain diverse data patterns that fail to fit with generic start, lower & upper parameters extracted from the entire dataset
```{r growth pool fits}
OD_pool_nest <- OD_meta %>%
  filter(Group == DayGroup) %>%
  filter(Bench == DayBench) %>%
  nest(data = c(DDMMYYYY_HHMM, Well, Row, Filename, E_hours, OD_600, logOD_600))

OD_pool_nest <- OD_pool_nest %>%
  mutate(FitLog = map(data, ~possibnlSLM(OD_600 ~ LogisticEqn(x = E_hours, Intercept, Mu, Pmax),
                                         data = .x,
                                         start = list(Intercept = min(.$OD_600, na.rm = TRUE), Mu = 0.1, Pmax = max(.$OD_600, na.rm = TRUE))
                                         )
  ),
  TidiedLog = map(FitLog, tidy),
  PredictLog = map(FitLog, augment)
  )

OD_pool_nest %>%
  unnest(PredictLog) %>%
  ggplot() + 
  geom_point(aes(x = E_hours, y = OD_600)) + 
    geom_line(aes(x = E_hours, y = `.fitted`)) + 
 # geom_ribbon(aes(x = E_hours, ymin =  `.fitted` - `.resid`, ymax =  `.fitted` + `.resid`), colour = "grey", fill = "grey") +
  facet_grid(col = vars(MysteryID), rows = vars(Temp_C)) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = sym("MysteryID"), breaks = NULL, labels = NULL)) +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = sym("Temp_c"), breaks = NULL, labels = NULL)) +
  labs(title = sym(DayGroup), subtitle = sym(DayBench)) +
  theme_bw()

Mu_pool_est <- OD_pool_nest %>% 
  dplyr::select(-c(data, FitLog, PredictLog)) %>%
  unnest(TidiedLog) %>%
  select(-statistic) %>%
  pivot_wider(names_from = term, values_from = c(estimate:p.value))

Mu_pool_est %>%
  ggplot() +
  geom_point(aes(x = MysteryID, y = estimate_Mu)) +
  geom_errorbar(aes(x = MysteryID, ymin = estimate_Mu - std.error_Mu,  ymax = estimate_Mu + std.error_Mu)) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  facet_grid(rows = vars(Temp_C)) +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = sym("Temp_c"), breaks = NULL, labels = NULL)) +
   labs(title = sym(DayGroup), subtitle = sym(DayBench)) +
  theme_bw()

```

# Results & Discussion
