---
title: "Import microbial plate growth data"
author: "Maximilian Berthold, Douglas A. Campbell"
date: "`r format(Sys.Date())`"
output:
  html_document:
    df_print: paged
    code_folding: hide
---

This .Rmd imports Molecular Device Fluorescence well plate files, generated using an absorbance scan protocol.
The size of the well plate does not matter; the import creates columns for RowColumn LetterNumber (A01 etc.) for the largest plate type in the list of files.
Import of smaller plate types results in NA in the non-existent RowColumn for that plate.

ToDo

Export simple 600 nm trajectories for analyses in Excel


```{r load libraries, echo=FALSE, message = FALSE, warning = FALSE} 
# libraries; Note check actual dependencies
library(tidyverse) #core tidyverse packages
library(googledrive) #accessing googledrive
library(googlesheets4) #accessing googlesheets
library(minpack.lm) #curve fitting
library(broom) #tidy model outputs
#library(photobiology) photobiology::w_length2rgb


```

Fix File Paths for Import once DataSets accumulate
```{r variable names for file import & processing MOLECULAR DEVICES}
Project <- "BIOL2201"

#deauthorizes access to googlesheet
gs4_deauth()

#read contents of MetaData from GoogleSheet
MetaData <- read_sheet("https://docs.google.com/spreadsheets/d/1FTKwYlJXd0ze9WwVXCvtwe5qkpN3_HNbeeM3hg7xnrI/edit#gid=0")

#set variables for file import & processing
DataPathMD <- file.path("RawData", "TestPlate", fsep = .Platform$file.sep)
DataOneDrive <- "OneDrive - Mount Allison University/BIOL2201_2024/StudentDataTest"

file_id <- ".txt"

DataOut <- "ProcessData"

FileEncodeMD <- "UTF-16LE" 
DelimiterMD <- "\t"
HeaderRowsMD <- 2

#list of files
OD_files <- list.files(path = DataPathMD, pattern = file_id, full.names = TRUE)

OD_files
#test for duplicate file names in chl_files
unique(duplicated(OD_files))

# MetaData <- readxl::read_excel(file.path("~/OneDrive - Mount Allison University/BIOL2201_2024/BIOL2201LabMaterials/MicrobialGrowth/WellPlateMetaDataTest.xlsx")) |>
#   mutate(Group = case_when(Day == "TUESDAY" ~ "TUES",
#                          Day == "WEDNESDAY" ~ "WED",
#                          Day == "THURSDAY" ~ "THURS"))




#MetaData <- file.path("OneDrive - Mount Allison University/BIOL2201_2024/BIOL2201LabMaterials/MicrobialGrowth/WellPlateMetaDataTest.xlsx")

#MetaData <- "https://docs.google.com/spreadsheets/d/1FTKwYlJXd0ze9WwVXCvtwe5qkpN3_HNbeeM3hg7xnrI/edit#gid=0"

```
```{r guess encoding}
guess_encoding(file = OD_files[1])

```

```{r read files}
#"id" parameter of read_delim might replace read_delim_plus
#a read function using tidyverse::read_delim that skips a fixed number of header rows, and adds columns to the dataframe containing the filename and the file creation date time.
# 
read_delim_plus <- function(flnm, delimiter, headerrows, fileencode){read_delim(flnm, delim = delimiter,  col_names = TRUE,  skip = headerrows, escape_double = FALSE,  locale = locale(encoding = fileencode), trim_ws = TRUE) |>
    mutate(Filename = flnm)
  }


OD_data <- OD_files |>
  map_df(~read_delim_plus(flnm = ., delimiter = DelimiterMD,  headerrows = HeaderRowsMD,  fileencode = FileEncodeMD))

```


```{r tidy OD_data}
OD_data <- OD_data |>
  select(-c("Temperature(Â¡C)", "Basic Endpoint Protocol", "...27")) |>
  rename(Wavelength_nm = Wavelength) |>
  separate(Filename, into = c("Path1", "Path2", "YYYYMMDD", "HHMM","Plate", "Temp_C", "txt"), sep = "([\\/\\_\\.])", remove = FALSE) |>
  select(-c("Path1", "Path2", "txt")) |>
  relocate(c("YYYYMMDD", "HHMM","Plate", "Temp_C")) |>
  unite(YYYYMMDD_HHMM,  YYYYMMDD:HHMM, sep = "_", remove = TRUE) |>
  mutate(YYYYMMDD_HHMM = ymd_hm(YYYYMMDD_HHMM)) |>
  mutate(Temp_C = as.numeric(Temp_C),
         Wavelength_nm = as.numeric(Wavelength_nm)) |>
  filter(!is.na(Wavelength_nm))
```


Add colour coding column vector
```{r colournm vector}
NM <- unique(OD_data$Wavelength_nm)
ColourNM = (photobiology::w_length2rgb(NM))
names(ColourNM) <- NM

#ColourNM
```


```{r test spectra}
OD_data |>
  filter(Wavelength_nm >= 450,
         Wavelength_nm <= 700) |>
  filter(Filename == OD_files[1]) |>
  ggplot() +
  geom_point(aes(x = Wavelength_nm, y = A1, colour = as.factor(Wavelength_nm)), show.legend = FALSE) + 
  scale_colour_manual(values = ColourNM) +
  facet_grid(cols = vars(Filename)) + 
  theme_bw()


# ggsave(filename = file.path("Plots", "SpectraTest.png"), plot = SpectraTest)
```

```{r E_hours}
OD_data <- OD_data |>
  group_by(Plate, Wavelength_nm) |>
  mutate(E_hours =  as.numeric((YYYYMMDD_HHMM - min(YYYYMMDD_HHMM))) / 3600, .after = YYYYMMDD_HHMM) |>
  ungroup()
```

```{r OD test plots}
OD_data |>
  filter(Wavelength_nm == 600) |>
  ggplot() +
  geom_line(aes(x = E_hours, y = A1)) + 
  theme_bw()

OD_data |> 
  filter(Wavelength_nm == 600) |>
  ggplot() +
  geom_line(aes(x = E_hours, y = log(A1))) + 
  theme_bw()

OD_data |> 
  filter(Wavelength_nm %in% c(500, 550, 600, 650, 750)) |>
  ggplot() +
  geom_line(aes(x = E_hours, y = log(A1), colour = as.factor(Wavelength_nm)), show.legend = FALSE) + 
  scale_colour_manual(values = ColourNM) +
  facet_grid(rows = vars(Wavelength_nm)) + 
  theme_bw()

```

Export wide format OD_data to OneDrive for Excel Analyses
Need to use old %>% magrittr::pipe for compatibility; base R |> does not work
Fix this once data folders accumulate; Day, Bench, Light
https://www.tidyverse.org/blog/2023/05/purrr-walk-this-way/

```{r}
# OD_data %>%
#   filter(Wavelength_nm == 600) %>%
#   unite("Group_Bench", Group, Bench, sep = "_") %>%
#   arrange(Temp_C) %>%
#   nest(.by = c(Group_Bench)) %>%
#   {walk2(.x = .$data, .y = .$Group_Bench, ~ write_csv(.x, file = file.path("..", "..", "..", "..", DataOneDrive, str_c(.y, ".csv"))))}

```


```{r long format}
OD_long <- OD_data |>
  pivot_longer(cols = -c(YYYYMMDD_HHMM:Wavelength_nm, Filename), names_to = "Well", values_to = "OD") |>
  separate(Well, into = c("Row", "Col"), sep = 1, remove = FALSE)

```

```{r OD facet plot}
OD_long |> ggplot() +
  geom_line(aes(x = E_hours, y = log(OD), colour = as.factor(Wavelength_nm)), show.legend = FALSE) + 
  facet_grid(rows = vars(Row), cols = vars(Col)) +
   scale_colour_manual(values = ColourNM) +
  theme_bw()


```

Need to Fix MetaData variable columns and File data columns for left join.
Need PlateID and Well in both MetaData and File data
```{r merge with MetaData}
# OD600Meta <- left_join(OD_long, MetaData, by = )
ODMeta <- OD_long

```


```{r save OD600Meta, echo=FALSE}
saveRDS(ODMeta, file.path(DataOut, 
paste(Project, "ODMeta.Rds", sep = "_"), fsep = .Platform$file.sep))

```

#https://stackoverflow.com/questions/72906809/read-and-write-excel-and-csv-files-from-sharepoint-via-r
#need to get metadata working to implement this export; should work.
```{r export OD600.csv to shared OneDrive folder}
#OD600Meta |>
  # #select(-c(Media_mL, Innoc_mL, Row, Col, logOD_600)) |>
  # pivot_wider(names_from = c(Well, Wavelength_nm, MysteryID), values_from = c(OD)) |>
   #pivot_wider(names_from = c(Well, Wavelength_nm), values_from = c(OD)) |>
  # arrange(Temp_C) |>
  # unite("Group_Bench", Group, Bench, sep = "_") |>
  # nest(.by = c(Group_Bench)) |>
  # {walk2(.x = .$data, .y = .$Group_Bench, ~ write_csv(.x, file = file.path("..", "..", "..", "..", DataOneDrive, str_c(.y, ".csv"))))}

```


Implement MultiSpectral Analyses and Growth Curve fits well by well.
Define equations as functions.
x will be taken from 'E_days' when we run the fit.
```{r logistic_eqn}
LogisticEqn <-  function(x, Pmax, Mu, Intercept){(Pmax*Intercept*exp(Mu*x))/(Pmax + (Intercept*(exp(Mu*x)-1)))
}

ModGompertzEqn <- function(x, Gmax,Gmu,Lag){(Gmax*(exp(-exp((Gmu*exp(1))/Gmax*(Lag-x)+1))))}

possibnlSLM = possibly(.f = nlsLM, otherwise = NULL)
```

Fit treatment specific logistic growth trajectories using nest purrr:map & broom::augment using nest specific start, lower & upper settings extracted from each nest on the fly.
This may be necessary if the 'nests' contain diverse data patterns that fail to fit with generic start, lower & upper parameters extracted from the entire dataset

```{r growth fits}
OD_nest <- ODMeta |>
  nest(data = c(YYYYMMDD_HHMM, Filename, E_hours, OD))

OD_nest <- OD_nest |> 
  mutate(FitLog = map(data, ~possibnlSLM(OD ~ LogisticEqn(x = E_hours, Intercept, Mu, Pmax),
                                         data = .x,
                                         start = list(Intercept = min(.$OD, na.rm = TRUE), Mu = 0.1, Pmax = max(.$OD, na.rm = TRUE))
                                         )
  ),
  PredictLog = map(FitLog, augment),
  TidiedLog = map(FitLog, tidy),
  ParamLog = map(FitLog, glance)
  )

OD_nest |>
  unnest(PredictLog) |>
  ggplot() + 
  #geom_point(aes(x = E_hours, y = OD)) +
  geom_point(aes(x = E_hours, y = OD, colour = as.factor(Wavelength_nm))) + 
  geom_ribbon(aes(x = E_hours, ymin =  `.fitted` - `.resid`, ymax =  `.fitted` + `.resid`, colour = as.factor(Wavelength_nm))) +
  scale_colour_manual(values = ColourNM) +
  facet_grid(row = vars(Row), col = vars(Col)) +
  theme_bw()

OD_nest |> 
  dplyr::select(-c(data, FitLog, PredictLog, ParamLog)) |>
  unnest(TidiedLog) |>
  select(-statistic) |>
  pivot_wider(names_from = term, values_from = c(estimate:p.value))

OD_nest |> 
  dplyr::select(-c(data, FitLog, PredictLog, ParamLog)) |>
  unnest(TidiedLog) |>
  select(-statistic) |>
  pivot_wider(names_from = term, values_from = c(estimate:p.value)) |>
  filter(Well == "A1") |>
  ggplot() +
  geom_point(aes(x = Wavelength_nm, y = estimate_Mu)) +
  theme_bw()

```

